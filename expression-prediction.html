<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Anshul Kundaje, Oana Ursu, and Sean Davis" />


<title>Expression Prediction</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />
<link href="site_libs/ionicons-2.0.1/css/ionicons.min.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->



<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">seandavi(s12): Courses and Tutorials</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="setup.html">
    <span class="fa fa-cogs"></span>
     
    setup
  </a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="ion ion-easel"></span>
     
    Slides
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="motivation_for_R_slides.html">Motivation for using R</a>
    </li>
    <li>
      <a href="http://bit.ly/bioc_cshl_2019">Introduction to Bioconductor</a>
    </li>
    <li>
      <a href="https://drive.google.com/file/d/1txUz-a84VVxiB1ouv24ujL2DSTfxgblL/view?usp=sharing">Advanced Bioconductor Overview</a>
    </li>
    <li>
      <a href="MachineLearning.html">Machine Learning hands-on</a>
    </li>
    <li>
      <a href="https://docs.google.com/presentation/d/1PKP39ze3kATKCXxx-AUuDdI4FUpA85UQJxDMhXIK3Mk/edit?usp=sharing">Machine Learning Intro</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    <span class="fa fa-question fa-lg"></span>
     
    Misc.
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="further_resources.html">Further resources</a>
    </li>
    <li>
      <a href="https://github.com/seandavi/ITR">Source code for this site</a>
    </li>
    <li>
      <a href="https://github.com/seandavi/ITR/archive/master.zip">Download materials</a>
    </li>
  </ul>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Expression Prediction</h1>
<h4 class="author">Anshul Kundaje, Oana Ursu, and Sean Davis</h4>
<h4 class="date">7/8/2018</h4>

</div>


<div id="background" class="section level1">
<h1>Background</h1>
<p>In this little set of exercises, you will be using histone marks near a gene to predict its expression.</p>
<p><span class="math display">\[y = h1 + h2 + h3 + ...\]</span></p>
<p>We will try a couple of different approaches:</p>
<ol style="list-style-type: decimal">
<li>Penalized regression</li>
<li>RandomForest</li>
</ol>
<div id="penalized-regression" class="section level2">
<h2>Penalized regression</h2>
<p>Adapted from <a href="http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/" class="uri">http://www.sthda.com/english/articles/37-model-selection-essentials-in-r/153-penalized-regression-essentials-ridge-lasso-elastic-net/</a>.</p>
<div id="ridge-regression" class="section level3">
<h3>Ridge regression</h3>
<p>Ridge regression shrinks the regression coefficients, so that variables, with minor contribution to the outcome, have their coefficients close to zero. The shrinkage of the coefficients is achieved by penalizing the regression model with a penalty term called L2-norm, which is the sum of the squared coefficients. The amount of the penalty can be fine-tuned using a constant called lambda (λ). Selecting a good value for λ is critical. When λ=0, the penalty term has no effect, and ridge regression will produce the classical least square coefficients. However, as λ increases to infinite, the impact of the shrinkage penalty grows, and the ridge regression coefficients will get close zero.</p>
<p>Note that, in contrast to the ordinary least square regression, ridge regression is highly affected by the scale of the predictors. Therefore, it is better to standardize (i.e., scale) the predictors before applying the ridge regression (James et al. 2014), so that all the predictors are on the same scale. The standardization of a predictor x, can be achieved using the formula x’ = x / sd(x), where sd(x) is the standard deviation of x. The consequence of this is that, all standardized predictors will have a standard deviation of one allowing the final fit to not depend on the scale on which the predictors are measured.</p>
<p>One important advantage of the ridge regression, is that it still performs well, compared to the ordinary least square method (Chapter <span class="citation">@ref</span>(linear-regression)), in a situation where you have a large multivariate data with the number of predictors (p) larger than the number of observations (n). One disadvantage of the ridge regression is that, it will include all the predictors in the final model, unlike the stepwise regression methods, which will generally select models that involve a reduced set of variables. Ridge regression shrinks the coefficients towards zero, but it will not set any of them exactly to zero. The lasso regression is an alternative that overcomes this drawback.</p>
</div>
<div id="lasso-regression" class="section level3">
<h3>Lasso regression</h3>
<p>Lasso stands for <em>Least Absolute Shrinkage and Selection Operator</em>. It shrinks the regression coefficients toward zero by penalizing the regression model with a penalty term called L1-norm, which is the sum of the absolute coefficients. In the case of lasso regression, the penalty has the effect of forcing some of the coefficient estimates, with a minor contribution to the model, to be exactly equal to zero. This means that, lasso can be also seen as an alternative to the subset selection methods for performing variable selection in order to reduce the complexity of the model. As in ridge regression, selecting a good value of λ for the lasso is critical.</p>
<p>One obvious advantage of lasso regression over ridge regression, is that it produces simpler and more interpretable models that incorporate only a reduced set of the predictors. However, neither ridge regression nor the lasso will universally dominate the other. Generally, lasso might perform better in a situation where some of the predictors have large coefficients, and the remaining predictors have very small coefficients. Ridge regression will perform better when the outcome is a function of many predictors, all with coefficients of roughly equal size (James et al. 2014).</p>
<p>Cross-validation methods can be used for identifying which of these two techniques is better on a particular data set.</p>
</div>
<div id="elastic-net" class="section level3">
<h3>Elastic Net</h3>
<p>Elastic Net produces a regression model that is penalized with both the L1-norm and L2-norm. The consequence of this is to effectively shrink coefficients (like in ridge regression) and to set some coefficients to zero (as in LASSO).</p>
</div>
</div>
</div>
<div id="install-packages" class="section level1">
<h1>Install packages</h1>
<p>Do this at the beginning of the lab</p>
<pre class="r"><code>install.packages(&quot;caret&quot;, dependencies=T)
install.packages(c(&quot;randomForest&quot;, &quot;glmnet&quot;), dependencies=T)</code></pre>
<pre class="r"><code>require(caret)</code></pre>
<pre><code>## Warning in library(package, lib.loc = lib.loc, character.only = TRUE,
## logical.return = TRUE, : there is no package called &#39;caret&#39;</code></pre>
<pre class="r"><code>require(randomForest)
require(glmnet)</code></pre>
<pre><code>## Warning in library(package, lib.loc = lib.loc, character.only = TRUE,
## logical.return = TRUE, : there is no package called &#39;glmnet&#39;</code></pre>
</div>
<div id="load-the-preprocessed-data" class="section level1">
<h1>Load the preprocessed data</h1>
<pre class="r"><code>fullFeatureSet &lt;- read.table(&quot;http://seandavi.github.io/ITR/expression-prediction/features.txt&quot;);
target &lt;- scan(url(&quot;http://seandavi.github.io/ITR/expression-prediction/target.txt&quot;), skip=1)</code></pre>
<p>You can see the full list of features in fullFeatureSet using:</p>
<pre class="r"><code>colnames(fullFeatureSet)</code></pre>
<pre><code>##  [1] &quot;Control&quot;  &quot;Dnase&quot;    &quot;H2az&quot;     &quot;H3k27ac&quot;  &quot;H3k27me3&quot; &quot;H3k36me3&quot;
##  [7] &quot;H3k4me1&quot;  &quot;H3k4me2&quot;  &quot;H3k4me3&quot;  &quot;H3k79me2&quot; &quot;H3k9ac&quot;   &quot;H3k9me1&quot; 
## [13] &quot;H3k9me3&quot;  &quot;H4k20me1&quot;</code></pre>
<p>Take a few minutes to try to understand the relationships between the predictor variables, their scale, etc.</p>
<ul>
<li><a href="https://web.stanford.edu/~hastie/TALKS/enet_talk.pdf" class="uri">https://web.stanford.edu/~hastie/TALKS/enet_talk.pdf</a></li>
</ul>
</div>
<div id="machine-learning" class="section level1">
<h1>Machine Learning</h1>
<div id="lasso" class="section level2">
<h2>Lasso</h2>
<p>In this section, we will run the lasso procedure.</p>
<pre class="r"><code>features &lt;- fullFeatureSet</code></pre>
<pre class="r"><code>library(caret)
#how to split into train/validation using cross-validation
fitControl &lt;- trainControl( 
    method=&quot;repeatedcv&quot;,
    number=10,
    ## repeated once
    repeats=1,
    verboseIter=T)</code></pre>
<p>We are going to try a number of different models, so here, we create a range of parameters to investigate.</p>
<ul>
<li>alpha - is the elasticnet mixing parameter: <span class="math display">\[\frac{(1-α)}{2}||β||_2^2+α||β||_1\]</span></li>
<li>lambda - is the regularization parameter governing the relative importance of minimizing error vs keeping betas small</li>
</ul>
<pre class="r"><code># setting alpha=1 specifies LASSO regression penalty
lassoGrid &lt;- expand.grid(alpha=1, lambda=10^seq(-6, 0, 1))</code></pre>
<p>Now, we train the model using cross-validation to find the “best” parameters.</p>
<pre class="r"><code>lassoFit &lt;- train(features, target, method=&quot;glmnet&quot;,
                  trControl=fitControl, tuneGrid=lassoGrid)</code></pre>
<pre><code>## + Fold01.Rep1: alpha=1, lambda=1 
## - Fold01.Rep1: alpha=1, lambda=1 
## + Fold02.Rep1: alpha=1, lambda=1 
## - Fold02.Rep1: alpha=1, lambda=1 
## + Fold03.Rep1: alpha=1, lambda=1 
## - Fold03.Rep1: alpha=1, lambda=1 
## + Fold04.Rep1: alpha=1, lambda=1 
## - Fold04.Rep1: alpha=1, lambda=1 
## + Fold05.Rep1: alpha=1, lambda=1 
## - Fold05.Rep1: alpha=1, lambda=1 
## + Fold06.Rep1: alpha=1, lambda=1 
## - Fold06.Rep1: alpha=1, lambda=1 
## + Fold07.Rep1: alpha=1, lambda=1 
## - Fold07.Rep1: alpha=1, lambda=1 
## + Fold08.Rep1: alpha=1, lambda=1 
## - Fold08.Rep1: alpha=1, lambda=1 
## + Fold09.Rep1: alpha=1, lambda=1 
## - Fold09.Rep1: alpha=1, lambda=1 
## + Fold10.Rep1: alpha=1, lambda=1 
## - Fold10.Rep1: alpha=1, lambda=1 
## Aggregating results
## Selecting tuning parameters
## Fitting alpha = 1, lambda = 0.001 on full training set</code></pre>
<pre class="r"><code>lassoModel &lt;- lassoFit$finalModel</code></pre>
<p>What metric is being used? Hint: print <code>names(lassoFit)</code> and get the metric used.</p>
<p>Printing the <code>lassoFit</code> variable gives the overall performance.</p>
<pre class="r"><code>names(lassoFit)</code></pre>
<pre><code>##  [1] &quot;method&quot;       &quot;modelInfo&quot;    &quot;modelType&quot;    &quot;results&quot;     
##  [5] &quot;pred&quot;         &quot;bestTune&quot;     &quot;call&quot;         &quot;dots&quot;        
##  [9] &quot;metric&quot;       &quot;control&quot;      &quot;finalModel&quot;   &quot;preProcess&quot;  
## [13] &quot;trainingData&quot; &quot;resample&quot;     &quot;resampledCM&quot;  &quot;perfNames&quot;   
## [17] &quot;maximize&quot;     &quot;yLimits&quot;      &quot;times&quot;        &quot;levels&quot;</code></pre>
<pre class="r"><code>print(lassoFit)</code></pre>
<pre><code>## glmnet 
## 
## 8641 samples
##   14 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 1 times) 
## Summary of sample sizes: 7777, 7777, 7777, 7777, 7777, 7777, ... 
## Resampling results across tuning parameters:
## 
##   lambda  RMSE  Rsquared  MAE 
##   1e-06   2.21  0.750     1.62
##   1e-05   2.21  0.750     1.62
##   1e-04   2.21  0.750     1.62
##   1e-03   2.21  0.750     1.62
##   1e-02   2.21  0.750     1.62
##   1e-01   2.23  0.744     1.64
##   1e+00   2.51  0.736     2.05
## 
## Tuning parameter &#39;alpha&#39; was held constant at a value of 1
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 1 and lambda = 0.001.</code></pre>
<ul>
<li>Accuracy per CV-fold (for best model)</li>
</ul>
<pre class="r"><code>print(lassoFit$resample)</code></pre>
<pre><code>##    RMSE Rsquared  MAE    Resample
## 1  2.18    0.760 1.55 Fold07.Rep1
## 2  2.20    0.753 1.61 Fold03.Rep1
## 3  2.25    0.742 1.65 Fold05.Rep1
## 4  2.19    0.752 1.62 Fold02.Rep1
## 5  2.17    0.756 1.61 Fold09.Rep1
## 6  2.30    0.723 1.69 Fold06.Rep1
## 7  2.26    0.737 1.68 Fold08.Rep1
## 8  2.19    0.765 1.60 Fold10.Rep1
## 9  2.13    0.760 1.56 Fold04.Rep1
## 10 2.19    0.752 1.63 Fold01.Rep1</code></pre>
<p>We can inspect the coefficients for different values of the L1 penalty lambda - play around and see what happens.</p>
<pre class="r"><code>print(coef(lassoModel, s=1e-4))</code></pre>
<pre><code>## 15 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                   1
## (Intercept) -4.4929
## Control     -0.1595
## Dnase        0.4740
## H2az         0.1990
## H3k27ac     -0.1992
## H3k27me3    -0.5880
## H3k36me3     0.6753
## H3k4me1     -0.0932
## H3k4me2     -0.1460
## H3k4me3      0.3357
## H3k79me2     0.6071
## H3k9ac       0.4594
## H3k9me1     -0.4166
## H3k9me3     -0.1905
## H4k20me1     0.0615</code></pre>
<pre class="r"><code>print(coef(lassoModel, s=1))</code></pre>
<pre><code>## 15 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                   1
## (Intercept) -3.6031
## Control      .     
## Dnase        0.3023
## H2az         .     
## H3k27ac      .     
## H3k27me3     .     
## H3k36me3     0.1044
## H3k4me1      .     
## H3k4me2      .     
## H3k4me3      0.0343
## H3k79me2     0.5703
## H3k9ac       0.2765
## H3k9me1      .     
## H3k9me3      .     
## H4k20me1     .</code></pre>
<p>We can also plot the entire regularization path The numbers shown are the feature (column) ids - to get the name of the feature, you can do colnames(features)[10], for instance. the numbers at the top are the numbers of nonzero coefficients.</p>
<pre class="r"><code>plot(lassoModel, &quot;lambda&quot;, label=T)</code></pre>
<p><img src="expression-prediction_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>The final, trained model is in <code>lassoFit</code>. We can plot the predictions vs original targets.</p>
<pre class="r"><code>lassoPreds &lt;- predict(lassoFit, newdata=features)
plot(target, lassoPreds)</code></pre>
<p><img src="expression-prediction_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
</div>
<div id="random-forests" class="section level2">
<h2>Random Forests</h2>
<pre class="r"><code>library(caret)</code></pre>
<pre class="r"><code>rfGrid &lt;- expand.grid(mtry=floor(ncol(features)/3))</code></pre>
<pre class="r"><code>randomforestFit &lt;- train(features, target, method=&quot;rf&quot;, 
                         trControl=fitControl, tuneGrid=rfGrid,
                         ntree=100)</code></pre>
<pre><code>## + Fold01.Rep1: mtry=4 
## - Fold01.Rep1: mtry=4 
## + Fold02.Rep1: mtry=4 
## - Fold02.Rep1: mtry=4 
## + Fold03.Rep1: mtry=4 
## - Fold03.Rep1: mtry=4 
## + Fold04.Rep1: mtry=4 
## - Fold04.Rep1: mtry=4 
## + Fold05.Rep1: mtry=4 
## - Fold05.Rep1: mtry=4 
## + Fold06.Rep1: mtry=4 
## - Fold06.Rep1: mtry=4 
## + Fold07.Rep1: mtry=4 
## - Fold07.Rep1: mtry=4 
## + Fold08.Rep1: mtry=4 
## - Fold08.Rep1: mtry=4 
## + Fold09.Rep1: mtry=4 
## - Fold09.Rep1: mtry=4 
## + Fold10.Rep1: mtry=4 
## - Fold10.Rep1: mtry=4 
## Aggregating results
## Fitting final model on full training set</code></pre>
<pre class="r"><code>rfModel &lt;- randomforestFit$finalModel</code></pre>
<p>The overall accuracy is:</p>
<pre class="r"><code>print(randomforestFit)</code></pre>
<pre><code>## Random Forest 
## 
## 8641 samples
##   14 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 1 times) 
## Summary of sample sizes: 7777, 7776, 7777, 7777, 7777, 7777, ... 
## Resampling results:
## 
##   RMSE  Rsquared  MAE 
##   2.13  0.767     1.51
## 
## Tuning parameter &#39;mtry&#39; was held constant at a value of 4</code></pre>
<p>We can also look at the accuracy per cross-validation fold.</p>
<pre class="r"><code>print(randomforestFit$resample)</code></pre>
<pre><code>##    RMSE Rsquared  MAE    Resample
## 1  2.14    0.758 1.52 Fold01.Rep1
## 2  2.08    0.775 1.44 Fold02.Rep1
## 3  2.08    0.780 1.48 Fold03.Rep1
## 4  2.19    0.747 1.56 Fold04.Rep1
## 5  2.10    0.776 1.49 Fold05.Rep1
## 6  2.19    0.756 1.55 Fold06.Rep1
## 7  2.11    0.772 1.51 Fold07.Rep1
## 8  2.14    0.764 1.51 Fold08.Rep1
## 9  2.23    0.753 1.56 Fold09.Rep1
## 10 2.07    0.788 1.46 Fold10.Rep1</code></pre>
<p>The variable importance gives a measure of the relative contributions of each of the variables (histone marks) to the expression prediction. Larger values reflect greater feature importance.</p>
<pre class="r"><code>print(rfModel$importance[order(rfModel$importance, decreasing=T),])</code></pre>
<pre><code>## H3k79me2   H3k9ac  H3k4me3  H3k27ac    Dnase H3k36me3  H3k4me2     H2az 
##    32437    27788    23553    20887    14632    12402    12054     4668 
## H3k27me3  H3k4me1  H3k9me1 H4k20me1  H3k9me3  Control 
##     3468     3251     2827     2720     2649     2510</code></pre>
<p>The final trained model is in randomforestFit. Again, we can plot the predictions vs original targets.</p>
<pre class="r"><code>randomforestPreds &lt;- predict(randomforestFit, newdata=features)
plot(target, randomforestPreds)</code></pre>
<p><img src="expression-prediction_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
</div>
</div>
<div id="further-exploration" class="section level1">
<h1>Further exploration</h1>
<ul>
<li>How does the performance of random forests compare to the lasso? (you can look at the output of print(lassoFit) and print(randomforestFit))</li>
<li><p>How do other models perform? You can try other models by changing the “method” parameter in the “train” call. Some suggestions for models: linear regression, “lm” and regression trees, “rpart2”.</p></li>
<li><p>Construct a proper test set and re-run the analyses</p></li>
<li><p>How do the individual histone marks contribute to the accuracy of the predictions? You can formulate hypotheses about which marks are important and only include those in the feature matrix when learning your model to see how they do. We provide some code below to help you with this.</p></li>
</ul>
<p>We can experiment with the weights that lasso regression produces when given a subset of the features. First, create a column vector specifying the names of a subset of the features with:</p>
<pre class="r"><code>featureSubset &lt;- c(&quot;Control&quot;, &quot;H3k4me1&quot;, &quot;H3k4me2&quot;, &quot;H2az&quot;, &quot;H3k27me3&quot;,
                   &quot;H3k36me3&quot;, &quot;H3k9me1&quot;, &quot;H3k9me3&quot;, &quot;H4k20me1&quot;)</code></pre>
<p>Now create the variable “features” which contains this subset of features:</p>
<pre class="r"><code>features &lt;- fullFeatureSet[featureSubset]</code></pre>
<p>Now, rerun the lasso regression with the subset.</p>
<pre class="r"><code>lassoFit &lt;- train(features, target, method=&quot;glmnet&quot;,
                  trControl=fitControl, tuneGrid=lassoGrid)</code></pre>
<pre><code>## + Fold01.Rep1: alpha=1, lambda=1 
## - Fold01.Rep1: alpha=1, lambda=1 
## + Fold02.Rep1: alpha=1, lambda=1 
## - Fold02.Rep1: alpha=1, lambda=1 
## + Fold03.Rep1: alpha=1, lambda=1 
## - Fold03.Rep1: alpha=1, lambda=1 
## + Fold04.Rep1: alpha=1, lambda=1 
## - Fold04.Rep1: alpha=1, lambda=1 
## + Fold05.Rep1: alpha=1, lambda=1 
## - Fold05.Rep1: alpha=1, lambda=1 
## + Fold06.Rep1: alpha=1, lambda=1 
## - Fold06.Rep1: alpha=1, lambda=1 
## + Fold07.Rep1: alpha=1, lambda=1 
## - Fold07.Rep1: alpha=1, lambda=1 
## + Fold08.Rep1: alpha=1, lambda=1 
## - Fold08.Rep1: alpha=1, lambda=1 
## + Fold09.Rep1: alpha=1, lambda=1 
## - Fold09.Rep1: alpha=1, lambda=1 
## + Fold10.Rep1: alpha=1, lambda=1 
## - Fold10.Rep1: alpha=1, lambda=1 
## Aggregating results
## Selecting tuning parameters
## Fitting alpha = 1, lambda = 0.001 on full training set</code></pre>
<pre class="r"><code>lassoModel &lt;- lassoFit$finalModel</code></pre>
<p>We now generate a plot where the y axis is the coefficient of the weights assigned to the various features by lasso, the bottom x-axis is the log of the regularisation parameter lambda, and the top x-axis is the number of non-zero weights for that particular value of the regularisation parameter. The numbers on the lines correspond to the indices of the features in “featureSubset”. The numbers at the top are the numbers of nonzero betas.</p>
<pre class="r"><code>plot(lassoModel, &quot;lambda&quot;, label=T)</code></pre>
<p><img src="expression-prediction_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>

    <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-93043521-1', 'auto');
        ga('send', 'pageview');

         
        var links = document.querySelectorAll('a');
        Array.prototype.map.call(links, function(item) {
            if (item.host != document.location.host) {
                item.addEventListener('click', function() {
                    var action = item.getAttribute('data-action') || 'follow';
                    ga('send', 'event', 'outbound', action, item.href);
                });
            }
        });
    </script>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
