---
title: "Machine Learning in R with mlr3"
author: "Sean Davis"
date: '2022-07-04'
output: html_document
---

# Background

- Supervised
  - Classification
  - Regression
- Unsupervised
  - "Clustering"
  - Dimensionality reduction


![](https://1.bp.blogspot.com/-ME24ePzpzIM/UQLWTwurfXI/AAAAAAAAANw/W3EETIroA80/s1600/drop_shadows_background.png)

## Specific methods

### Linear regression

In [statistics](https://en.wikipedia.org/wiki/Statistics "Statistics"), **linear regression** is a [linear](https://en.wikipedia.org/wiki/Linearity "Linearity") approach for modelling the relationship between a [scalar](https://en.wikipedia.org/wiki/Scalar_(mathematics) "Scalar (mathematics)") response and one or more explanatory variables (also known as [dependent and independent variables](https://en.wikipedia.org/wiki/Dependent_and_independent_variables "Dependent and independent variables")). The case of one explanatory variable is called _[simple linear regression](https://en.wikipedia.org/wiki/Simple_linear_regression "Simple linear regression")_; for more than one, the process is called **multiple linear regression**. This term is distinct from [multivariate linear regression](https://en.wikipedia.org/wiki/Multivariate_linear_regression "Multivariate linear regression"), where multiple [correlated](https://en.wikipedia.org/wiki/Correlation_and_dependence "Correlation and dependence") dependent variables are predicted, rather than a single scalar variable.

In linear regression, the relationships are modeled using [linear predictor functions](https://en.wikipedia.org/wiki/Linear_predictor_function "Linear predictor function") whose unknown model [parameters](https://en.wikipedia.org/wiki/Parameters "Parameters") are [estimated](https://en.wikipedia.org/wiki/Estimation_theory "Estimation theory") from the [data](https://en.wikipedia.org/wiki/Data "Data"). Such models are called [linear models](https://en.wikipedia.org/wiki/Linear_model "Linear model"). Most commonly, the [conditional mean](https://en.wikipedia.org/wiki/Conditional_expectation "Conditional expectation") of the response given the values of the explanatory variables (or predictors) is assumed to be an [affine function](https://en.wikipedia.org/wiki/Affine_transformation "Affine transformation") of those values; less commonly, the conditional [median](https://en.wikipedia.org/wiki/Median "Median") or some other [quantile](https://en.wikipedia.org/wiki/Quantile "Quantile") is used. Like all forms of [regression analysis](https://en.wikipedia.org/wiki/Regression_analysis "Regression analysis"), linear regression focuses on the [conditional probability distribution](https://en.wikipedia.org/wiki/Conditional_probability_distribution "Conditional probability distribution") of the response given the values of the predictors, rather than on the [joint probability distribution](https://en.wikipedia.org/wiki/Joint_probability_distribution "Joint probability distribution") of all of these variables, which is the domain of [multivariate analysis](https://en.wikipedia.org/wiki/Multivariate_analysis "Multivariate analysis").

Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.

### K-nearest Neighbor

```{r echo=FALSE, fig.cap="**Figure**. The k-nearest neighbor algorithm can be used for regression or classification. "}
knitr::include_graphics('https://www.kdnuggets.com/wp-content/uploads/rapidminer-knn-image1.jpg')
```


In [statistics](https://en.wikipedia.org/wiki/Statistics "Statistics"), the **_k_\-nearest neighbors algorithm** (**_k_\-NN**) is a [non-parametric](https://en.wikipedia.org/wiki/Non-parametric_statistics "Non-parametric statistics") [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning "Supervised learning") method first developed by [Evelyn Fix](https://en.wikipedia.org/wiki/Evelyn_Fix "Evelyn Fix") and [Joseph Hodges](https://en.wikipedia.org/wiki/Joseph_Lawson_Hodges_Jr. "Joseph Lawson Hodges Jr.") in 1951, and later expanded by [Thomas Cover](https://en.wikipedia.org/wiki/Thomas_M._Cover "Thomas M. Cover"). It is used for [classification](https://en.wikipedia.org/wiki/Statistical_classification "Statistical classification") and [regression](https://en.wikipedia.org/wiki/Regression_analysis "Regression analysis"). In both cases, the input consists of the _k_ closest training examples in a [data set](https://en.wikipedia.org/wiki/Data_set "Data set"). The output depends on whether _k_\-NN is used for classification or regression:

- In _k-NN classification_, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its _k_ nearest neighbors (_k_ is a positive [integer](https://en.wikipedia.org/wiki/Integer "Integer"), typically small). If _k_ = 1, then the object is simply assigned to the class of that single nearest neighbor.

- In _k-NN regression_, the output is the property value for the object. This value is the average of the values of _k_ nearest neighbors.

_k_\-NN is a type of [classification](https://en.wikipedia.org/wiki/Classification "Classification") where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then [normalizing](https://en.wikipedia.org/wiki/Normalization_(statistics) "Normalization (statistics)") the training data can improve its accuracy dramatically.

Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/_d_, where _d_ is the distance to the neighbor.

The neighbors are taken from a set of objects for which the class (for _k_\-NN classification) or the object property value (for _k_\-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.


### Classification and Regression Trees (CART)

[Decision Tree Learning](https://en.wikipedia.org/wiki/Decision_tree_learning) is supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations. Tree models where the target variable can take a discrete set of values are called _classification trees_; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called _regression trees_.

```{r echo=FALSE,fig.cap='**Figure**. An example of a decision tree that performs classification, also sometimes called a classification tree. '}
knitr::include_graphics('https://upload.wikimedia.org/wikipedia/commons/e/eb/Decision_Tree.jpg')
```

### RandomForest

**Random forests** or **random decision forests** is an [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning "Ensemble learning") method for [classification](https://en.wikipedia.org/wiki/Statistical_classification "Statistical classification"), [regression](https://en.wikipedia.org/wiki/Regression_analysis "Regression analysis") and other tasks that operates by constructing a multitude of [decision trees](https://en.wikipedia.org/wiki/Decision_tree_learning "Decision tree learning") at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned. Random decision forests correct for decision trees' habit of [overfitting](https://en.wikipedia.org/wiki/Overfitting "Overfitting") to their [training set](https://en.wikipedia.org/wiki/Test_set "Test set"). Random forests generally outperform [decision trees](https://en.wikipedia.org/wiki/Decision_tree_learning "Decision tree learning"), but their accuracy is lower than gradient boosted trees\[_[citation needed](https://en.wikipedia.org/wiki/Wikipedia:Citation_needed "Wikipedia:Citation needed")_\]. However, data characteristics can affect their performance.

The first algorithm for random decision forests was created in 1995 by [Tin Kam Ho](https://en.wikipedia.org/wiki/Tin_Kam_Ho "Tin Kam Ho") using the [random subspace method](https://en.wikipedia.org/wiki/Random_subspace_method "Random subspace method"), which, in Ho's formulation, is a way to implement the "stochastic discrimination" approach to classification proposed by Eugene Kleinberg.

An extension of the algorithm was developed by [Leo Breiman](https://en.wikipedia.org/wiki/Leo_Breiman "Leo Breiman") and [Adele Cutler](https://en.wikipedia.org/wiki/Adele_Cutler "Adele Cutler"), who registered "Random Forests" as a [trademark](https://en.wikipedia.org/wiki/Trademark "Trademark") in 2006 (as of 2019[\[update\]](https://en.wikipedia.org/w/index.php?title=Random_forest&action=edit), owned by [Minitab, Inc.](https://en.wikipedia.org/wiki/Minitab "Minitab")). The extension combines Breiman's "[bagging](https://en.wikipedia.org/wiki/Bootstrap_aggregating "Bootstrap aggregating")" idea and random selection of features, introduced first by Ho and later independently by Amit and [Geman](https://en.wikipedia.org/wiki/Donald_Geman "Donald Geman") in order to construct a collection of decision trees with controlled variance.

Random forests are frequently used as "blackbox" models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration.


```{r echo=FALSE, fig.cap="**Figure**. Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time." }
knitr::include_graphics('https://upload.wikimedia.org/wikipedia/commons/7/76/Random_forest_diagram_complete.png')
```

# Working with Data

## mlr3

```{r echo=FALSE, fig.cap="**Figure**. The mlr3 ecosystem is rich and growing quickly"}
knitr::include_graphics('https://raw.githubusercontent.com/mlr-org/mlr3/master/man/figures/mlr3verse.svg')
```

### Tasks

Tasks are objects that contain the (usually tabular) data and additional meta-data to define a machine learning problem. The meta-data is, for example, the name of the target variable for supervised machine learning problems, or the type of the dataset (e.g. a spatial or survival task). This information is used by specific operations that can be performed on a task.

---
Tasks are objects that contain the (usually tabular) data and additional meta-data to define a machine learning problem. The meta-data is, for example, the name of the target variable for supervised machine learning problems, or the type of the dataset (e.g. a _spatial_ or _survival_ task). This information is used by specific operations that can be performed on a task.

#### Task Types[](https://mlr3book.mlr-org.com/02-basics-tasks.html#tasks-types)

To create a task from a [`data.frame()`](https://www.rdocumentation.org/packages/base/topics/data.frame), [`data.table()`](https://www.rdocumentation.org/packages/data.table/topics/data.table-package) or [`Matrix()`](https://www.rdocumentation.org/packages/Matrix/topics/Matrix), you first need to select the right task type:

-   **Classification Task**: The target is a label (stored as `character` or `factor`) with only relatively few distinct values → [`TaskClassif`](https://mlr3.mlr-org.com/reference/TaskClassif.html).
    
-   **Regression Task**: The target is a numeric quantity (stored as `integer` or `numeric`) → [`TaskRegr`](https://mlr3.mlr-org.com/reference/TaskRegr.html).
    
-   **Survival Task**: The target is the (right-censored) time to an event. More censoring types are currently in development → [`mlr3proba::TaskSurv`](https://mlr3proba.mlr-org.com/reference/TaskSurv.html) in add-on package [mlr3proba](https://mlr3proba.mlr-org.com/).
    
-   **Density Task**: An unsupervised task to estimate the density → [`mlr3proba::TaskDens`](https://mlr3proba.mlr-org.com/reference/TaskDens.html) in add-on package [mlr3proba](https://mlr3proba.mlr-org.com/).
    
-   **Cluster Task**: An unsupervised task type; there is no target and the aim is to identify similar groups within the feature space → [`mlr3cluster::TaskClust`](https://mlr3cluster.mlr-org.com/reference/TaskClust.html) in add-on package [mlr3cluster](https://mlr3cluster.mlr-org.com/).
    
-   **Spatial Task**: Observations in the task have spatio-temporal information (e.g. coordinates) → [`mlr3spatiotempcv::TaskRegrST`](https://mlr3spatiotempcv.mlr-org.com/reference/TaskRegrST.html) or [`mlr3spatiotempcv::TaskClassifST`](https://mlr3spatiotempcv.mlr-org.com/reference/TaskClassifST.html) in add-on package [mlr3spatiotempcv](https://mlr3spatiotempcv.mlr-org.com/).
    
-   **Ordinal Regression Task**: The target is ordinal → `TaskOrdinal` in add-on package [mlr3ordinal](https://github.com/mlr-org/mlr3ordinal) (still in development).

### Learners

Objects of class [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) provide a unified interface to many popular machine learning algorithms in R. They consist of methods to train and predict a model for a [`Task`](https://mlr3.mlr-org.com/reference/Task.html) and provide meta-information about the learners, such as the hyperparameters (which control the behavior of the learner) you can set.

The base class of each learner is [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html), specialized for regression as [`LearnerRegr`](https://mlr3.mlr-org.com/reference/LearnerRegr.html) and for classification as [`LearnerClassif`](https://mlr3.mlr-org.com/reference/LearnerClassif.html). Other types of learners, provided by extension packages, also inherit from the [`Learner`](https://mlr3.mlr-org.com/reference/Learner.html) base class, e.g. [`mlr3proba::LearnerSurv`](https://mlr3proba.mlr-org.com/reference/LearnerSurv.html) or [`mlr3cluster::LearnerClust`](https://mlr3cluster.mlr-org.com/reference/LearnerClust.html).

All Learners work in a two-stage procedure:

```{r echo=FALSE}
knitr::include_graphics('https://mlr3book.mlr-org.com/images/learner.svg')
```

-   **Training stage**: The training data (features and target) is passed to the Learner’s `$train()` function which trains and stores a model, i.e. the relationship of the target and features.
-   **Predict stage**: The new data, usually a different slice of the original data than used for training, is passed to the `$predict()` method of the Learner. The model trained in the first step is used to predict the missing target, e.g. labels for classification problems or the numerical value for regression problems.

#### Predefined Learners[](https://mlr3book.mlr-org.com/02-basics-learners.html#predefined-learners)

The [mlr3](https://mlr3.mlr-org.com/) package ships with the following set of classification and regression learners. We deliberately keep this small to avoid unnecessary dependencies:

-   [`classif.featureless`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.featureless.html): Simple baseline classification learner. The default is to always predict the label that is most frequent in the training set. While this is not very useful by itself, it can be used as a “[fallback learner](https://mlr3book.mlr-org.com/fallback-learners)” to make predictions in case another, more sophisticated, learner failed for some reason.
-   [`regr.featureless`](https://mlr3.mlr-org.com/reference/mlr_learners_regr.featureless.html): Simple baseline regression learner. The default is to always predict the mean of the target in training set. Similar to [`mlr_learners_classif.featureless`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.featureless.html), it makes for a good “[fallback learner](https://mlr3book.mlr-org.com/fallback-learners)”
-   [`classif.rpart`](https://mlr3.mlr-org.com/reference/mlr_learners_classif.rpart.html): Single classification tree from package [rpart](https://cran.r-project.org/package=rpart).
-   [`regr.rpart`](https://mlr3.mlr-org.com/reference/mlr_learners_regr.rpart.html): Single regression tree from package [rpart](https://cran.r-project.org/package=rpart).

This set of baseline learners is usually insufficient for a real data analysis. Thus, we have cherry-picked implementations of the most popular machine learning method and collected them in the [mlr3learners](https://mlr3learners.mlr-org.com/) package:

-   Linear ([`regr.lm`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.lm.html)) and logistic ([`classif.log_reg`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.log_reg.html)) regression
-   Penalized Generalized Linear Models ([`regr.glmnet`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.glmnet.html), [`classif.glmnet`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.glmnet.html)), possibly with built-in optimization of the penalization parameter ([`regr.cv_glmnet`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.cv_glmnet.html), [`classif.cv_glmnet`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.cv_glmnet.html))
-   (Kernelized) k\-Nearest Neighbors regression ([`regr.kknn`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.kknn.html)) and classification ([`classif.kknn`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.kknn.html)).
-   Kriging / Gaussian Process Regression ([`regr.km`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.km.html))
-   Linear ([`classif.lda`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.lda.html)) and Quadratic ([`classif.qda`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.qda.html)) Discriminant Analysis
-   Naive Bayes Classification ([`classif.naive_bayes`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.naive_bayes.html))
-   Support-Vector machines ([`regr.svm`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.svm.html), [`classif.svm`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.svm.html))
-   Gradient Boosting ([`regr.xgboost`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.xgboost.html), [`classif.xgboost`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.xgboost.html))
-   Random Forests for regression and classification ([`regr.ranger`](https://mlr3learners.mlr-org.com/reference/mlr_learners_regr.ranger.html), [`classif.ranger`](https://mlr3learners.mlr-org.com/reference/mlr_learners_classif.ranger.html))

More machine learning methods and alternative implementations are collected in the [mlr3extralearners repository](https://github.com/mlr-org/mlr3extralearners/).

### mlr3 Workflow

1. Load data
2. Split data into training and test sets
3. Create a [task](https://mlr3book.mlr-org.com/02-basics-tasks.html). 
4. Choose a [learner](https://mlr3book.mlr-org.com/02-basics-learners.html). See `mlr_learners`.
5. Train
6. Predict
7. Assess
8. Interpret


## Setup

```{r message=FALSE}
library(mlr3verse)
library(GEOquery)
library(mlr3learners) # for knn
library(ranger) # for randomforest
```

## Classification

### The Data

The data we are going to access are from [this paper](https://doi.org/10.1158/1055-9965.EPI-17-0461). 

Background: The tumor microenvironment is an important factor in cancer immunotherapy response. To further understand how a tumor affects the local immune system, we analyzed immune gene expression differences between matching normal and tumor tissue.

Methods: We analyzed public and new gene expression data from solid cancers and isolated immune cell populations. We also determined the correlation between CD8, FoxP3 IHC, and our gene signatures.

Results: We observed that regulatory T cells (Tregs) were one of the main drivers of immune gene expression differences between normal and tumor tissue. A tumor-specific CD8 signature was slightly lower in tumor tissue compared with normal of most (12 of 16) cancers, whereas a Treg signature was higher in tumor tissue of all cancers except liver. Clustering by Treg signature found two groups in colorectal cancer datasets. The high Treg cluster had more samples that were consensus molecular subtype 1/4, right-sided, and microsatellite-instable, compared with the low Treg cluster. Finally, we found that the correlation between signature and IHC was low in our small dataset, but samples in the high Treg cluster had significantly more CD8+ and FoxP3+ cells compared with the low Treg cluster.

Conclusions: Treg gene expression is highly indicative of the overall tumor immune environment.

Impact: In comparison with the consensus molecular subtype and microsatellite status, the Treg signature identifies more colorectal tumors with high immune activation that may benefit from cancer immunotherapy. 

#### Data Preparation

Use the [GEOquery] package to fetch data about [GSE103512].


```{r geoquery10, echo=TRUE, cache=TRUE, message=FALSE}
library(GEOquery)
gse = getGEO("GSE103512")[[1]]
```

The first step, a detail, is to convert from the older Bioconductor data structure (GEOquery was written in 2007), the `ExpressionSet`, to the newer `SummarizedExperiment`.

```{r message=FALSE}
library(SummarizedExperiment)
se = as(gse, "SummarizedExperiment")
```

Examine two variables of interest, cancer type and tumor/normal status.

```{r geoquery20,echo=TRUE,cache=TRUE}
with(colData(se),table(`cancer.type.ch1`,`normal.ch1`))
```

#### Feature selection

Filter gene expression by variance to find most informative genes.

```{r sds,cache=TRUE,echo=TRUE}
sds = apply(assay(se, 'exprs'),1,sd)
# filter out normal tissues
se_small = se[order(sds,decreasing = TRUE)[1:200],
              colData(se)$characteristics_ch1.1=='normal: no']
dat = assay(se_small, 'exprs')
```

```{r}
feat_dat = t(dat)
tumor = data.frame(tumor_type = colData(se_small)$cancer.type.ch1, feat_dat)
```

### Creating the "task"

```{r}
tumor$tumor_type = as.factor(tumor$tumor_type)
task = as_task_classif(tumor,target='tumor_type')
```

Here, we randomly divide the data into 2/3 training data and 1/3 test data.

```{r}
set.seed(7)
train_set = sample(task$row_ids, 0.67 * task$nrow)
test_set = setdiff(task$row_ids, train_set)
```

### K-nearest-neighbor

```{r}
learner = lrn("classif.kknn")
```

#### Train

```{r}
learner$train(task, row_ids = train_set)
```

Here, we can look at the trained model:

```{r}
learner$model
```

#### Predict

Let's use our trained model works to predict the classes of the 
**training** data. 

```{r}
pred_train = learner$predict(task, row_ids=train_set)
```

And check on the 

```{r}
pred_test = learner$predict(task, row_ids=test_set)
```

#### Assess

In this section, we can look at the accuracy and performance of our model 
on the training data and the test data. 

```{r}
pred_train$confusion
```

```{r}
measures = msrs(c('classif.acc'))
pred_train$score(measures)
```

```{r}
pred_test$confusion
pred_test$score(measures)
```

### Classification tree

#### Train

```{r}
learner = lrn("classif.rpart")
```

```{r}
learner$train(task, row_ids = train_set)
```

```{r}
learner$model
```

#### Predict

```{r}
pred_train = learner$predict(task, row_ids=train_set)
```

```{r}
pred_test = learner$predict(task, row_ids=test_set)
```

#### Assess

```{r}
pred_train$confusion
```

```{r}
measures = msrs(c('classif.acc'))
pred_train$score(measures)
```

```{r}
pred_test$confusion
pred_test$score(measures)
```



### RandomForest

```{r}
learner = lrn("classif.ranger", importance = "impurity")
```

#### Train

```{r}
learner$train(task, row_ids = train_set)
```

```{r}
learner$model
```

#### Predict

```{r}
pred_train = learner$predict(task, row_ids=train_set)
```

```{r}
pred_test = learner$predict(task, row_ids=test_set)
```

#### Assess

```{r}
pred_train$confusion
```

```{r}
measures = msrs(c('classif.acc'))
pred_train$score(measures)
```

```{r}
pred_test$confusion
pred_test$score(measures)
```

The randomforest procedure gives us a measure of the "importance" of each variable.
We can look at those importances (just think of them as ranks, not "values") to
rank genes that contribute the most to the prediction capacity of the model. 

```{r}
variab_filter = flt("importance", learner = learner)
variab_filter$calculate(task)
head(as.data.table(variab_filter), 10)
```

## Regression

### The Data

We will be building a regression model for chronological age prediction, based on DNA methylation. This is based on the work of [Jana Naue et al. 2017](https://www.sciencedirect.com/science/article/pii/S1872497317301643?via%3Dihub), in which biomarkers are examined to predict the chronological age of humans by analyzing the DNA methylation patterns. Different machine learning algorithms are used in this study to make an age prediction.

It has been recognized that within each individual, the level of [DNA methylation](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3174260/) changes with age. This knowledge is used to select useful biomarkers from DNA methylation datasets. The [CpG sites](https://en.wikipedia.org/wiki/CpG_site) with the highest correlation to age are selected as the biomarkers (and therefore features for building a regression model). In this tutorial, specific biomarkers are analyzed by machine learning algorithms to create an age prediction model.

The data are taken from [this tutorial](https://training.galaxyproject.org/training-material/topics/statistics/tutorials/regression_machinelearning/tutorial.html). 



```{r}
library(data.table)
meth_age = rbind(
    fread('https://zenodo.org/record/2545213/files/test_rows_labels.csv'),
    fread('https://zenodo.org/record/2545213/files/train_rows.csv')
)
```

```{r}
task = as_task_regr(meth_age,target = 'Age')
```

```{r}
set.seed(7)
train_set = sample(task$row_ids, 0.67 * task$nrow)
test_set = setdiff(task$row_ids, train_set)
```



### Linear regression

```{r}
learner = lrn("regr.lm")
```

#### Train

```{r}
learner$train(task, row_ids = train_set)
```

#### Predict

```{r}
pred_train = learner$predict(task, row_ids=train_set)
```

```{r}
pred_test = learner$predict(task, row_ids=test_set)
```

#### Assess

```{r}
pred_train
```

We can plot the relationship between the `truth` and `response`, or predicted
value to see visually how our model performs.

```{r}
library(ggplot2)
ggplot(pred_train,aes(x=truth, y=response)) +
    geom_point() +
    geom_smooth(method='lm')
```

We can use the r-squared of the fit to roughly compare two models. 

```{r}
measures = msrs(c('regr.rsq'))
pred_train$score(measures)
```

```{r}
pred_test
pred_test$score(measures)
```

### Regression tree

```{r}
learner = lrn("regr.rpart")
```

#### Train

```{r}
learner$train(task, row_ids = train_set)
```

```{r}
learner$model
```

What is odd about using a regression tree here is that we end up with only a few
discrete estimates of age. Each "leaf" has a value. 

#### Predict

```{r}
pred_train = learner$predict(task, row_ids=train_set)
```

```{r}
pred_test = learner$predict(task, row_ids=test_set)
```

#### Assess

```{r}
pred_train
```

We can see the effect of the discrete values much more clearly here. 

```{r}
library(ggplot2)
ggplot(pred_train,aes(x=truth, y=response)) +
    geom_point() +
    geom_smooth(method='lm')
```

And the r-squared values for this model prediction shows quite a bit of difference
from the linear regression above. 

```{r}
measures = msrs(c('regr.rsq'))
pred_train$score(measures)
```

```{r}
pred_test
pred_test$score(measures)
```

### RandomForest

Randomforest is also tree-based, but unlike the single regression tree above,
randomforest is a "forest" of trees which will eliminate the discrete nature
of a single tree. 

```{r}
learner = lrn("regr.ranger")
```

#### Train

```{r}
learner$train(task, row_ids = train_set)
```

```{r}
learner$model
```

#### Predict

```{r}
pred_train = learner$predict(task, row_ids=train_set)
```

```{r}
pred_test = learner$predict(task, row_ids=test_set)
```

#### Assess

```{r}
pred_train
```

```{r}
ggplot(pred_train,aes(x=truth, y=response)) +
    geom_point() +
    geom_smooth(method='lm')
```

```{r}
measures = msrs(c('regr.rsq'))
pred_train$score(measures)
```

```{r}
pred_test
pred_test$score(measures)
```
